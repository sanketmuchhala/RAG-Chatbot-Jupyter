### 7) Your turn: practice exercises

Try these quick exercises:
- Change the `temperature` to make answers more creative (only when using OpenAI).
- Edit the prompt in the first chain to enforce a word limit.
- Add more facts to the RAG corpus and re-run the question.
- Swap the retriever `k` value and see how it changes the answer.
- Build another structured schema (e.g., `Task(name, priority, due_date)`) and extract it from a sentence.

Bonus ideas:
- Load a real PDF with `PyPDFLoader` and plug it into the RAG index.
- Replace embeddings with another provider (e.g., local) if you have one available.def format_docs(docs):
    return "\n\n".join(d.page_content for d in docs)

rag_prompt = ChatPromptTemplate.from_messages([
    ("system", "Use the context to answer the question. If you don't know, say you don't know.\n\nContext:\n{context}"),
    ("human", "{question}")
])

rag_chain = (
    {"context": retriever | RunnableLambda(format_docs), "question": RunnablePassthrough()} 
    | rag_prompt 
    | model 
    | StrOutputParser()
)

rag_chain.invoke("What is LangChain and why are embeddings useful?")from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import FakeEmbeddings

texts = [
    "LangChain helps build apps with LLMs using composable prompts, chains, and tools.",
    "FAISS is a library for efficient similarity search over vector embeddings.",
    "Embeddings convert text into vectors so similar meanings are close in vector space.",
    "Retrieval-Augmented Generation combines search over a knowledge base with generation.",
]

splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)
rag_docs = splitter.create_documents(texts)

if USE_OPENAI:
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
else:
    embeddings = FakeEmbeddings(size=1536)

vector = FAISS.from_documents(rag_docs, embeddings)
retriever = vector.as_retriever(search_type="similarity", search_kwargs={"k": 2})

print(f"Built FAISS index with {len(rag_docs)} chunks.")### 6) Tiny Retrieval‑Augmented Generation (RAG)

We'll build a mini knowledge base, embed it, retrieve relevant chunks, and have the model answer using the retrieved context. We'll use FAISS for the vector index.from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    MessagesPlaceholder("history"),
    ("human", "{input}")
])

base_chat = chat_prompt | model | StrOutputParser()

session_store = {}

def get_session_history(session_id: str):
    if session_id not in session_store:
        session_store[session_id] = InMemoryChatMessageHistory()
    return session_store[session_id]

chat = RunnableWithMessageHistory(
    base_chat,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history",
)

cfg = {"configurable": {"session_id": "demo-session"}}
print(chat.invoke({"input": "Hi! I'm Sam."}, config=cfg))
print(chat.invoke({"input": "What's my name?"}, config=cfg))### 5) Conversational memory (multi‑turn)

We'll keep a running history so the model can remember prior turns. This uses `RunnableWithMessageHistory` and an in-memory store.from typing import Literal
from pydantic import BaseModel, Field
import json, re

class ReviewAnalysis(BaseModel):
    sentiment: Literal["positive", "neutral", "negative"]
    aspects: list[str] = Field(default_factory=list)
    summary: str

text = "The laptop is fast but the battery life is mediocre; the screen is excellent."

if USE_OPENAI:
    structured_model = model.with_structured_output(ReviewAnalysis)
    result = structured_model.invoke({"input": text})
    result
else:
    # Fallback: prompt the toy model to produce JSON, then try to parse
    json_prompt = ChatPromptTemplate.from_messages([
        ("system", "Extract sentiment (positive|neutral|negative), aspects (comma list), and a 1-sentence summary as JSON with keys: sentiment, aspects, summary."),
        ("human", "Text: {text}")
    ])
    chain = json_prompt | model | StrOutputParser()
    raw = chain.invoke({"text": text})
    print("Model output:\n", raw)
    try:
        parsed = json.loads(re.search(r"\{[\s\S]*\}", raw).group(0))
        print("\nParsed JSON:", parsed)
    except Exception:
        print("\n(JSON parsing best-effort; structure may vary with toy model)")### 4) Structured output (JSON)

When available, we can ask the model to return a validated Pydantic object. If we're using the toy model, we'll demonstrate a JSON-style output prompt and parse best-effort.from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a concise assistant that explains things simply."),
    ("human", "In one sentence, explain: {topic}")
])

simple_chain = prompt | model | StrOutputParser()

simple_chain.invoke({"topic": "LangChain's LCEL in plain terms"})### 3) First chain: prompt → model → output parser

A small, readable pipeline using the LangChain Expression Language (LCEL):
- `ChatPromptTemplate`: formats inputs into messages
- `model`: the LLM or our toy fallback
- `StrOutputParser`: returns plain text# Setup: env, imports, and model factory
import os
import getpass
from dotenv import load_dotenv

load_dotenv()

# Optionally prompt for key if not set; safe to press Enter and skip
if not os.getenv("OPENAI_API_KEY"):
    try:
        os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter OPENAI_API_KEY (or press Enter to skip): ") or ""
    except Exception:
        pass

USE_OPENAI = bool(os.getenv("OPENAI_API_KEY"))

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# Create a model: OpenAI if available, else a simple local toy model

def make_model():
    if USE_OPENAI:
        return ChatOpenAI(model="gpt-4o-mini", temperature=0)

    # Fallback: a tiny toy model that turns messages into a short echo string
    def to_text(messages):
        try:
            return messages.to_string()
        except Exception:
            return str(messages)

    return RunnableLambda(lambda messages: "[ToyModel] " + to_text(messages)[-500:])

model = make_model()
print("Using OpenAI:", USE_OPENAI)### 2) Setup: environment, imports, and model selection

We'll load environment variables, import LangChain components, and choose a model.
- If `OPENAI_API_KEY` is available, we'll use `gpt-4o-mini` via `ChatOpenAI`.
- Otherwise, we'll use a tiny local "toy model" so you can still practice the chaining workflow.pip install -qU "langchain>=0.2.14" "langchain-community>=0.2.10" "langchain-openai>=0.1.22" "langchain-text-splitters>=0.2.2" faiss-cpu tiktoken python-dotenv pypdf pandas### 1) Install dependencies

This cell installs the libraries used below. You can skip it if your environment already has them.
- `langchain`, `langchain-community`, `langchain-openai`: core SDK and providers
- `tiktoken`: tokenizer utilities
- `faiss-cpu`: fast vector index for RAG
- `pypdf`: to read PDFs (for RAG demo)
- `pandas`: tabular data utils
- `python-dotenv`: load `.env` secrets
## LangChain Notebook Practice: A Human‑Friendly Guide

Welcome! This notebook is a hands-on, well-documented walkthrough to help you practice core LangChain concepts in a friendly, human-written style.

What you'll do:
- Understand the building blocks: models, prompts, chains, memory
- Create useful chains with `ChatPromptTemplate` and output parsers
- Try basic memory for multi-turn conversations
- Build a tiny Retrieval-Augmented Generation (RAG) demo

Prerequisites:
- Python 3.10+
- An OpenAI API key (optional but recommended). If not set, the notebook will fall back to a tiny “toy model” so you can still practice the workflow.

Tips:
- Run cells top-to-bottom the first time
- If you change package versions, occasionally restart the kernel
- Skim the big-picture headings; then dive into the code cells and play!