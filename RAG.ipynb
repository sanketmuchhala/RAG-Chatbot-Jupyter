{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53fd124-3780-4a8d-a116-426281776bc8",
   "metadata": {},
   "source": [
    "#  RAG Chatbot in Jupyter using DeepSeek API\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) chatbot using your own `.pdf`, `.docx`, or `.txt` files.  \n",
    "It uses:\n",
    "- FAISS for semantic search\n",
    "- sentence-transformers` for local embeddings\n",
    "- DeepSeek Chat API for final answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e643ab51-d7ba-432f-81c9-a55fa931781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install faiss-cpu tiktoken openai python-docx pdfplumber PyMuPDF sentence-transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0420e4-c609-4ccd-922a-daab3b1ac351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd78f50b-264c-4a31-9d0d-0e3624664a96",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Set API Key\n",
    "We import all the core libraries and set your DeepSeek API key for use in later steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1887fcea-4096-4c1e-a67e-470a479808ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import docx\n",
    "import faiss\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import requests\n",
    "from typing import List\n",
    "from IPython.display import display, Markdown\n",
    "from sklearn.preprocessing import normalize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set your DeepSeek API Key\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = \"ENTER_YOUR_DEEPSEEK_API_KEY\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeadeb4-9166-4deb-ac9e-85cb9d30bec3",
   "metadata": {},
   "source": [
    "## ðŸ“„ Step 3: Load a Text Document\n",
    "We support `.pdf`, `.docx`, and `.txt`.  \n",
    "The file content will be extracted as plain text for chunking and embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f55cd405-0e72-4612-ac40-bcc9110810a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to your file (.pdf/.docx/.txt):  /Users/sanketmuchhala/Downloads/llm.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file: /Users/sanketmuchhala/Downloads/llm.pdf\n",
      "Sample: I. Understanding Language Models\n",
      "Chapter 1: An Introduction to Large Language \n",
      "Models\n",
      "This chapter provides a comprehensive overview of Large Language Models (LLMs) and the \n",
      "evolution of Language AI, marking humanity's inflection point with AI systems capable of \n",
      "human-like text generation.\n",
      "The Evolution of Language AI\n",
      "The chapter traces the development from simple bag-of-words representations in the 1950s to \n",
      "today's sophisticated models:\n",
      "â€¢\n",
      "Bag-of-Words (1950s-2000s): Simple word counting appro\n"
     ]
    }
   ],
   "source": [
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "# Function to extract text from supported file types\n",
    "def extract_text_from_file(filepath: str) -> str:\n",
    "    if filepath.endswith(\".pdf\"):\n",
    "        doc = fitz.open(filepath)\n",
    "        return \"\\n\".join([page.get_text() for page in doc])\n",
    "    elif filepath.endswith(\".docx\"):\n",
    "        doc = docx.Document(filepath)\n",
    "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    elif filepath.endswith(\".txt\"):\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "# Prompt user to enter file path manually\n",
    "filepath = input(\"Enter the path to your file (.pdf/.docx/.txt): \")\n",
    "raw_text = extract_text_from_file(filepath)\n",
    "print(f\"Loaded file: {filepath}\")\n",
    "print(f\"Sample: {raw_text[:500]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238058f-3272-47a5-ae15-7db41aa11955",
   "metadata": {},
   "source": [
    "## Step 4: Chunk the Document Text\n",
    "We split the full text into overlapping chunks to preserve context for semantic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc61be18-4ccd-4d62-9360-e095415ad4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Split into 22 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Split text into overlapping chunks using token count\n",
    "def split_text(text, max_tokens=500, overlap=50):\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(words):\n",
    "        chunk = words[i:i + max_tokens]\n",
    "        tokens = tokenizer.encode(\" \".join(chunk))\n",
    "        while len(tokens) > max_tokens:\n",
    "            chunk = chunk[:-1]\n",
    "            tokens = tokenizer.encode(\" \".join(chunk))\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        i += max_tokens - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = split_text(raw_text)\n",
    "print(f\" Split into {len(chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0dd216-7281-4169-b7aa-4b04a305e98e",
   "metadata": {},
   "source": [
    "## Step 5: Generate Embeddings Locally\n",
    "We use a free, fast local embedding model (`all-MiniLM-L6-v2`) to convert chunks into vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f60b6443-0c10-4394-af68-a1129d202a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved embeddings for 22 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Use SentenceTransformers to get local embeddings (no API cost)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')  # small and fast\n",
    "\n",
    "def get_local_embeddings(texts: List[str]) -> List[List[float]]:\n",
    "    return embedder.encode(texts, convert_to_numpy=True).tolist()\n",
    "\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), 5):  # batch of 5 for safety\n",
    "        batch = texts[i:i+5]\n",
    "        response = requests.post(url, headers=headers, json={\n",
    "            \"model\": \"deepseek-embedding-2\",\n",
    "            \"input\": batch\n",
    "        })\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Embedding API Error: {response.text}\")\n",
    "        data = response.json()\n",
    "        for item in data[\"data\"]:\n",
    "            embeddings.append(item[\"embedding\"])\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "embeddings = get_local_embeddings(chunks)\n",
    "print(f\"Retrieved embeddings for {len(embeddings)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bd202b-06f3-46ee-84f0-98f7bcd97b68",
   "metadata": {},
   "source": [
    "##  Step 6: Store Embeddings in FAISS\n",
    "We use FAISS to store all chunk vectors for efficient similarity search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c35cc0f-bcd2-474f-a41c-9163d2c7e572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 22 vectors in FAISS.\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS index and add embeddings\n",
    "dimension = len(embeddings[0])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "embeddings_np = np.array(embeddings).astype(\"float32\")\n",
    "index.add(embeddings_np)\n",
    "\n",
    "print(f\"Stored {index.ntotal} vectors in FAISS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f637c1b-30c7-48d1-8f13-19cab47f4131",
   "metadata": {},
   "source": [
    "## Step 7: Ask a Question & Retrieve Top Chunks\n",
    "We encode your question, perform vector search in FAISS, and retrieve the top relevant chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5604a446-2536-4afb-a9d9-ddc45b4cfd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question:  tell me about evolution of AI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Retrieved Chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "I. Understanding Language Models Chapter 1: An Introduction to Large Language Models This chapter provides a comprehensive overview of Large Language Models (LLMs) and the evolution of Language AI, marking humanity's inflection point with AI systems capable of human-like text generation. The Evolution of Language AI The chapter traces the development from simple bag-of-words representations in the\n",
      "\n",
      "--- Chunk 2 ---\n",
      "fine-tuning pretrained text generation models to adapt them for specific tasks and behaviors. Fine-tuning transforms base models into more useful, instruction-following systems through two main approaches: supervised fine-tuning and preference tuning. The Three LLM Training Steps 1. Language Modeling (Pretraining) â€“ Base models are pretrained on massive text datasets using next-token prediction. â€“\n",
      "\n",
      "--- Chunk 3 ---\n",
      "musical score, visual effects, ambition, themes, and emotional weight. It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics. Since its premiere, Interstellar gained a cult following,[5] and now is regarded by many sci-fi experts as one of the best science-fiction films of all time. Interstellar was nominated for five awards at the \n",
      "\n",
      "--- Chunk 4 ---\n",
      "focusing on text generation models and the mechanics of generative LLMs. Initial Setup Code import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # Load model and tokenizer tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k- instruct\") model = AutoModelForCausalLM.from_pretrained( \"microsoft/Phi-3-mini-4k-instruct\", device_map=\"cuda\", torch_dtype=\"auto\n"
     ]
    }
   ],
   "source": [
    "# Assume this is already run earlier:\n",
    "# document_embeddings = get_local_embeddings(chunks)\n",
    "# faiss_index.add(np.array(document_embeddings).astype(\"float32\"))\n",
    "\n",
    "# Perform similarity search using user question\n",
    "def retrieve_top_k(query: str, k=4):\n",
    "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(query_embedding.astype(\"float32\"), k)\n",
    "    return [chunks[i] for i in I[0]]\n",
    "\n",
    "# Ask a question\n",
    "question = input(\"Ask a question: \")\n",
    "top_chunks = retrieve_top_k(question)\n",
    "\n",
    "# Display retrieved context\n",
    "print(\"Top Retrieved Chunks:\")\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\\n{chunk[:400]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f004373-1922-4247-b9ba-4989ec46574d",
   "metadata": {},
   "source": [
    "## Step 8: Query DeepSeek with Retrieved Context\n",
    "We combine the relevant chunks and send them to DeepSeek Chat API for a grounded answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a8f73f9-c72f-4e9f-a69d-a24e95637f69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### DeepSeek Answer:\n",
       "\n",
       "The evolution of AI, particularly in the context of language models, has seen significant advancements over the decades. Here's a breakdown based on the provided context:\n",
       "\n",
       "### **Key Stages in the Evolution of Language AI**  \n",
       "1. **Bag-of-Words (1950sâ€“2000s)**  \n",
       "   - Early approach focused on counting word occurrences in text.  \n",
       "   - Ignored semantic meaning and context.  \n",
       "\n",
       "2. **Dense Vector Embeddings (2013)**  \n",
       "   - Introduced by **Word2vec**, which used neural networks to represent words as vectors.  \n",
       "   - Captured semantic relationships (e.g., \"king â€“ man + woman â‰ˆ queen\").  \n",
       "\n",
       "3. **Attention Mechanisms (2014â€“2017)**  \n",
       "   - Enhanced Recurrent Neural Networks (RNNs) by allowing models to focus on relevant parts of input text.  \n",
       "   - Paved the way for the **Transformer architecture** (Vaswani et al., 2017), which revolutionized NLP.  \n",
       "\n",
       "4. **Modern LLMs (2018+)**  \n",
       "   - Two dominant architectures emerged:  \n",
       "     - **Encoder-only models (e.g., BERT)**: Optimized for understanding language (e.g., classification, semantic search).  \n",
       "     - **Decoder-only models (e.g., GPT)**: Specialized in generative tasks (e.g., text completion).  \n",
       "   - Scale exploded (e.g., GPT-1 had 117M parameters; GPT-3 reached 175B).  \n",
       "\n",
       "### **Training Paradigms**  \n",
       "- **Pretraining**: Self-supervised learning on vast text corpora (next-token prediction).  \n",
       "- **Fine-Tuning**: Adapts models to specific tasks:  \n",
       "  - **Supervised Fine-Tuning (SFT)**: Uses labeled data for instruction-following.  \n",
       "  - **Preference Tuning**: Aligns outputs with human preferences (e.g., safety, quality).  \n",
       "- **Efficient Methods**:  \n",
       "  - **Parameter-Efficient Fine-Tuning (PEFT)**: Updates only small subsets of parameters (e.g., LoRA, QLoRA).  \n",
       "  - **Quantization**: Reduces computational costs (e.g., 4-bit precision with QLoRA).  \n",
       "\n",
       "### **Applications & Challenges**  \n",
       "- **Applications**: Chatbots, document retrieval, multimodal systems, and more.  \n",
       "- **Considerations**: Addressing bias, transparency, harmful content, and intellectual property.  \n",
       "\n",
       "This progression reflects a shift from rule-based systems to sophisticated, general-purpose AI capable of human-like text generation and understanding."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Function to call DeepSeek API and get an answer using retrieved context\n",
    "def ask_deepseek(question: str, context: str) -> str:\n",
    "    api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"DeepSeek API key not set. Please set it using os.environ['DEEPSEEK_API_KEY'].\")\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Limit context to avoid token overflow\n",
    "    max_context_chars = 6000\n",
    "    context = context[:max_context_chars]\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful assistant. Use the following context to answer the question.\n",
    "    \n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    url = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-chat\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a knowledgeable assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        return result.strip()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"API request failed:\", e)\n",
    "        print(\"Full response text:\", response.text if 'response' in locals() else \"No response.\")\n",
    "        return \"Failed to get a response from DeepSeek.\"\n",
    "\n",
    "# Combine chunks and ask a question\n",
    "combined_context = \"\\n\".join(top_chunks)\n",
    "answer = ask_deepseek(question, combined_context)\n",
    "\n",
    "# Display result\n",
    "if answer:\n",
    "    display(Markdown(f\"### DeepSeek Answer:\\n\\n{answer}\"))\n",
    "else:\n",
    "    print(\"No answer received.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95462c8-0ad9-4d70-b2b5-7d459f0e18ff",
   "metadata": {},
   "source": [
    "## ðŸ’¬ Step 9: Display the Answer\n",
    "Finally, we show the generated answer using the retrieved context from your file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18a5f05b-9cc5-4973-9b74-187e6f6e3001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### ðŸ¤– DeepSeek Answer:\n",
       "\n",
       "The provided text appears to be a collection of code snippets and explanations related to various advanced techniques in natural language processing (NLP) and machine learning. It covers topics such as:\n",
       "\n",
       "1. **Embeddings and Semantic Search**:  \n",
       "   - Using embeddings (e.g., with `co.embed`) for document retrieval and building a search index with FAISS.  \n",
       "   - Comparing keyword-based search (BM25) with semantic search.\n",
       "\n",
       "2. **Prompt Engineering**:  \n",
       "   - Techniques for improving LLM outputs, such as iterative refinement, modular design, and reasoning enhancement (e.g., chain-of-thought).  \n",
       "   - Methods for structured output generation (e.g., JSON constraints using `llama-cpp-python`).\n",
       "\n",
       "3. **Advanced Text Generation**:  \n",
       "   - Loading quantized models (e.g., GGUF format) for efficient inference.  \n",
       "   - Using LangChain for model I/O, chains (e.g., prompt templates), and extending LLM capabilities.\n",
       "\n",
       "4. **Text Clustering and Topic Modeling**:  \n",
       "   - Unsupervised techniques for grouping similar texts using embeddings (e.g., `SentenceTransformer`), dimensionality reduction (e.g., UMAP), and clustering (e.g., HDBSCAN).\n",
       "\n",
       "### Likely Source:  \n",
       "This is likely a chapter or section from a **technical book, course material, or research documentation** focused on NLP, LLMs, or machine learning. The content is advanced and practical, with code examples and explanations tailored for practitioners.\n",
       "\n",
       "### File Type:  \n",
       "It could be part of a:  \n",
       "- **Jupyter Notebook** (mix of code and markdown).  \n",
       "- **Technical report or whitepaper**.  \n",
       "- **Online tutorial or blog post series**.  \n",
       "\n",
       "The file itself might be a `.ipynb`, `.md`, `.txt`, or `.py` file, depending on how it was saved."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Join retrieved context\n",
    "combined_context = \"\\n\".join(top_chunks)\n",
    "\n",
    "# Call DeepSeek API with your question and retrieved document context\n",
    "answer = ask_deepseek(question, combined_context)\n",
    "\n",
    "# Display nicely formatted markdown output\n",
    "if answer:\n",
    "    display(Markdown(f\"###  DeepSeek Answer:\\n\\n{answer}\"))\n",
    "else:\n",
    "    print(\" No answer received.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
